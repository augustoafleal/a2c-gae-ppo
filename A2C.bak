import numpy as np
import torch
import torch.nn as nn
from torch import optim


class A2C(nn.Module):
    def __init__(
        self,
        n_features: int,
        n_actions: int,
        device: torch.device,
        critic_lr: float,
        actor_lr: float,
        n_envs: int,
        ppo_epochs: int,
        ppo_batch_size: int,
        use_ppo: bool = True,
        clip_coef: float = 0.2,
    ) -> None:
        super().__init__()
        self.device = device
        self.n_envs = n_envs
        self.use_ppo = use_ppo
        self.clip_coef = clip_coef
        self.hidden_size = 64  # Alterado para escalar em vez de tupla
        self.obs_space = n_features

        # Definir arquitetura das redes
        critic_layers = [
            nn.Linear(n_features, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, 1),
        ]

        actor_layers = [
            nn.Linear(n_features, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, n_actions),
        ]

        # Construir redes
        self.critic = nn.Sequential(*critic_layers).to(self.device)
        self.actor = nn.Sequential(*actor_layers).to(self.device)

        # Otimizadores (após mover redes para o device)
        self.critic_optim = optim.Adam(self.critic.parameters(), lr=critic_lr)
        self.actor_optim = optim.Adam(self.actor.parameters(), lr=actor_lr)

        # Parâmetros PPO
        self.ppo_batch_size = ppo_batch_size
        self.ppo_epochs = ppo_epochs

    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:
        x = torch.as_tensor(x, dtype=torch.float32, device=self.device)
        if x.ndim == 1:
            x = x.unsqueeze(0)  # Adiciona dimensão de batch se necessário
        state_values = self.critic(x)
        action_logits_vec = self.actor(x)
        return (state_values, action_logits_vec)

    def select_action(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        state_values, action_logits = self.forward(x)
        dist = torch.distributions.Categorical(logits=action_logits)
        actions = dist.sample()
        action_log_probs = dist.log_prob(actions)
        entropy = dist.entropy()
        return (actions, action_log_probs, state_values, entropy)

    def update_agent(self, rollouts, hp):
        """
        Atualiza o agente usando PPO (ou A2C se use_ppo=False)
        rollouts: dict com states, actions, rewards, value_preds, old_log_probs, masks
        """
        T, N, obs_dim = rollouts["states"].shape
        device = rollouts["states"].device

        # --- Calcula vantagens com GAE corrigido ---
        advantages = torch.zeros(T, N, device=device)
        gae = 0.0
        for t in reversed(range(T)):
            if t == T - 1:
                next_value = 0.0  # Assume terminal state
            else:
                next_value = rollouts["value_preds"][t + 1] * rollouts["masks"][t]

            delta = rollouts["rewards"][t] + hp["gamma"] * next_value - rollouts["value_preds"][t]
            gae = delta + hp["gamma"] * hp["lam"] * rollouts["masks"][t] * gae
            advantages[t] = gae

        returns = advantages + rollouts["value_preds"]

        # Normalizar vantagens (com proteção contra divisão por zero)
        advantages_mean = advantages.mean()
        advantages_std = advantages.std()
        if advantages_std > 0:
            advantages = (advantages - advantages_mean) / (advantages_std + 1e-8)
        else:
            advantages = advantages - advantages_mean

        # Achatar os tensores
        states_flat = rollouts["states"].reshape(-1, obs_dim)
        actions_flat = rollouts["actions"].reshape(-1)
        returns_flat = returns.reshape(-1).detach()
        old_log_probs_flat = rollouts["old_log_probs"].reshape(-1).detach()
        advantages_flat = advantages.reshape(-1).detach()

        if self.use_ppo:
            # PPO: múltiplas epochs e minibatches
            total_size = states_flat.size(0)
            actor_loss_total = 0
            critic_loss_total = 0
            entropy_total = 0
            num_updates = 0

            for _ in range(self.ppo_epochs):
                perm = torch.randperm(total_size)
                for start in range(0, total_size, self.ppo_batch_size):
                    end = start + self.ppo_batch_size
                    idx = perm[start:end]

                    batch_states = states_flat[idx]
                    batch_actions = actions_flat[idx]
                    batch_returns = returns_flat[idx]
                    batch_adv = advantages_flat[idx]
                    batch_old_log_probs = old_log_probs_flat[idx]

                    # --- Forward pass ---
                    new_values, new_logits = self.forward(batch_states)
                    dist = torch.distributions.Categorical(logits=new_logits)
                    new_log_probs = dist.log_prob(batch_actions)
                    entropy = dist.entropy().mean()

                    # --- PPO surrogate loss ---
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratio * batch_adv
                    surr2 = torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef) * batch_adv
                    actor_loss = -torch.min(surr1, surr2).mean() - hp["ent_coef"] * entropy

                    # --- Critic loss ---
                    critic_loss = (batch_returns - new_values.squeeze(-1)).pow(2).mean()

                    # --- Update ---
                    self.actor_optim.zero_grad()
                    self.critic_optim.zero_grad()
                    total_loss = actor_loss + critic_loss
                    total_loss.backward()
                    self.actor_optim.step()
                    self.critic_optim.step()

                    actor_loss_total += actor_loss.item()
                    critic_loss_total += critic_loss.item()
                    entropy_total += entropy.item()
                    num_updates += 1

            # Calcular médias
            if num_updates > 0:
                return (critic_loss_total / num_updates, actor_loss_total / num_updates, entropy_total / num_updates)
            else:
                return 0, 0, 0

        else:
            # A2C simples
            new_values, new_logits = self.forward(states_flat)
            dist = torch.distributions.Categorical(logits=new_logits)
            new_log_probs = dist.log_prob(actions_flat)
            entropy = dist.entropy().mean()

            actor_loss = -(advantages_flat * new_log_probs).mean() - hp["ent_coef"] * entropy
            critic_loss = (returns_flat - new_values.squeeze(-1)).pow(2).mean()

            # --- Atualiza ator ---
            self.actor_optim.zero_grad()
            actor_loss.backward()
            self.actor_optim.step()

            # --- Atualiza crítico ---
            self.critic_optim.zero_grad()
            critic_loss.backward()
            self.critic_optim.step()

            return critic_loss.item(), actor_loss.item(), entropy.item()

    """
    def get_losses(
        self,
        rewards: torch.Tensor,
        old_log_probs: torch.Tensor,
        value_preds: torch.Tensor,
        states: torch.Tensor,
        actions: torch.Tensor,
        masks: torch.Tensor,
        gamma: float,
        lam: float,
        ent_coef: float,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        T, N = rewards.shape
        device = rewards.device

        # --- GAE ---
        advantages = torch.zeros(T, N, device=device)
        gae = 0.0
        for t in reversed(range(T - 1)):
            delta = rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]
            gae = delta + gamma * lam * masks[t] * gae
            advantages[t] = gae

        returns = advantages + value_preds
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        adv_detached = advantages.detach()

        # Flatten para PPO
        states_flat = states.reshape(-1, self.obs_space)
        actions_flat = actions.reshape(-1)
        returns_flat = returns.reshape(-1)
        old_log_probs_flat = old_log_probs.reshape(-1).detach()
        advantages_flat = adv_detached.reshape(-1)
        ppo_epochs = self.ppo_epochs
        ppo_batch_size = self.ppo_batch_size
        if self.use_ppo:
            # PPO com múltiplas epochs e minibatches
            total_size = states_flat.size(0)
            actor_loss_epoch = 0
            critic_loss_epoch = 0
            entropy_epoch = 0
            for _ in range(self.ppo_epochs):
                perm = torch.randperm(total_size)
                for start in range(0, total_size, self.ppo_batch_size):
                    idx = perm[start : start + self.ppo_batch_size]

                    batch_states = states_flat[idx]
                    batch_actions = actions_flat[idx]
                    batch_returns = returns_flat[idx]
                    batch_adv = advantages_flat[idx]
                    batch_old_log_probs = old_log_probs_flat[idx]

                    # --- Forward pass ---
                    new_values, new_logits = self.forward(batch_states)
                    dist = torch.distributions.Categorical(logits=new_logits)
                    new_log_probs = dist.log_prob(batch_actions)
                    entropy = dist.entropy().mean()

                    # --- Actor ---
                    ratio = torch.exp(new_log_probs - batch_old_log_probs)
                    surr1 = ratio * batch_adv
                    surr2 = torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef) * batch_adv
                    actor_loss = -torch.min(surr1, surr2).mean() - ent_coef * entropy

                    # --- Critic ---
                    critic_loss = (batch_returns - new_values.squeeze(-1)).pow(2).mean()

                    actor_loss_epoch += actor_loss
                    critic_loss_epoch += critic_loss
                    entropy_epoch += entropy

            # Média das perdas
            actor_loss_epoch /= ppo_epochs * (total_size / ppo_batch_size)
            critic_loss_epoch /= ppo_epochs * (total_size / ppo_batch_size)
            entropy_epoch /= ppo_epochs * (total_size / ppo_batch_size)
            return critic_loss_epoch, actor_loss_epoch, entropy_epoch

        else:
            # --- A2C: 1 update simples ---
            _, new_logits = self.forward(states.reshape(-1, states.shape[-1]))
            dist = torch.distributions.Categorical(logits=new_logits)
            new_log_probs = dist.log_prob(actions.reshape(-1))
            entropy = dist.entropy().mean()

            actor_loss = -(adv_detached.reshape(-1) * new_log_probs).mean() - entropy * ent_coef
            critic_loss = (returns.reshape(-1) - value_preds.reshape(-1)).pow(2).mean()
            return critic_loss, actor_loss, entropy

    """